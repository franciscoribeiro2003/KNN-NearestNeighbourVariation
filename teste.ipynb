{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from scipy.spatial.distance import euclidean, cityblock, cosine, jensenshannon\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import auxiliarfunctions as af\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseEstimator:\n",
    "    y_required = True\n",
    "    fit_required = True\n",
    "\n",
    "    def _setup_input(self, X, y=None):\n",
    "        \"\"\"Ensure inputs to an estimator are in the expected format.\n",
    "\n",
    "        Ensures X and y are stored as numpy ndarrays by converting from an\n",
    "        array-like object if necessary. Enables estimators to define whether\n",
    "        they require a set of y target values or not with y_required, e.g.\n",
    "        kmeans clustering requires no target labels and is fit against only X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like\n",
    "            Feature dataset.\n",
    "        y : array-like\n",
    "            Target values. By default is required, but if y_required = false\n",
    "            then may be omitted.\n",
    "        \"\"\"\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = np.array(X)\n",
    "\n",
    "        if X.size == 0:\n",
    "            raise ValueError(\"Got an empty matrix.\")\n",
    "\n",
    "        if X.ndim == 1:\n",
    "            self.n_samples, self.n_features = 1, X.shape\n",
    "        else:\n",
    "            self.n_samples, self.n_features = X.shape[0], np.prod(X.shape[1:])\n",
    "\n",
    "        lof = LocalOutlierFactor(n_neighbors=20)\n",
    "        \n",
    "        # lof_scores_train = lof.negative_outlier_factor_\n",
    "        # lof_scores_train = (lof_scores_train - min(lof_scores_train)) / (max(lof_scores_train) - min(lof_scores_train))\n",
    "        self.weigth = lof.fit_predict(X)\n",
    "        self.X = X\n",
    "\n",
    "        if self.y_required:\n",
    "            if y is None:\n",
    "                raise ValueError(\"Missed required argument y\")\n",
    "\n",
    "            if not isinstance(y, np.ndarray):\n",
    "                y = np.array(y)\n",
    "\n",
    "            if y.size == 0:\n",
    "                raise ValueError(\"The targets array must be no-empty.\")\n",
    "\n",
    "        self.y = y\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self._setup_input(X, y)\n",
    "\n",
    "    def predict(self, X=None):\n",
    "        #  print(\"-----\")\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = np.array(X)\n",
    "\n",
    "        if self.X is not None or not self.fit_required:\n",
    "            return self._predict(X)\n",
    "        else:\n",
    "            raise ValueError(\"You must call `fit` before `predict`\")\n",
    "\n",
    "    def _predict(self, X=None):\n",
    "        raise NotImplementedError()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "\n",
    "class KNNBase(BaseEstimator):\n",
    "    def __init__(self, k=5, distance_func = None):\n",
    "        \"\"\"Base class for Nearest neighbors classifier and regressor.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        k : int, default 5\n",
    "            The number of neighbors to take into account. If 0, all the\n",
    "            training examples are used.\n",
    "        distance_func : function, default euclidean distance\n",
    "            A distance function taking two arguments. Any function from\n",
    "            scipy.spatial.distance will do.\n",
    "        \"\"\"\n",
    "\n",
    "        self.k = None if k == 0 else k  # l[:None] returns the whole list\n",
    "        self.distance_func = distance_func\n",
    "        self.distance = [euclidean, cityblock, cosine, jensenshannon]\n",
    "        \n",
    "\n",
    "    def aggregate(self, neighbors_targets):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _predict(self, X=None):\n",
    "        lof = LocalOutlierFactor(n_neighbors=20, algorithm='kd_tree')\n",
    "        train_weigth = lof.fit_predict(X)\n",
    "        print(train_weigth)\n",
    "        \n",
    "        predictions = [self._predict_x(x,weigth) for x, weigth in zip(X,train_weigth)]\n",
    "        predictions = [Counter(prediction).most_common(1)[0][0] for prediction in predictions]\n",
    "        print(predictions)\n",
    "        \n",
    "        return np.array(predictions)\n",
    "\n",
    "    def _predict_x(self, x, train_weigth):\n",
    "        \"\"\"Predict the label of a single instance x.\"\"\"\n",
    "        prediction = []\n",
    "        if self.distance_func is None:\n",
    "            for k in self.distance:\n",
    "\n",
    "                # compute distances between x and all examples in the training set.\n",
    "                distances = (k(x, example) for example in self.X)\n",
    "\n",
    "                # Sort all examples by their distance to x and keep their target value.\n",
    "                neighbors = sorted(((dist, target,weigth) for (dist, target,weigth) in zip(distances, self.y, self.weigth)), key=lambda x: x[0])\n",
    "\n",
    "                # print(\"Neighbors with distance:\", neighbors[: self.k])\n",
    "                # Get targets of the k-nn and aggregate them (most common one or\n",
    "                # average).\n",
    "\n",
    "                neighbors_targets = [(target,weigth) for (_, target, weigth) in neighbors[: self.k]]\n",
    "\n",
    "                prediction.append(self.aggregate(neighbors_targets,train_weigth))\n",
    "\n",
    "        return prediction\n",
    "\n",
    "\n",
    "class KNNClassifier(KNNBase):\n",
    "    \"\"\"Nearest neighbors classifier.\n",
    "\n",
    "    Note: if there is a tie for the most common label among the neighbors, then\n",
    "    the predicted label is arbitrary.\"\"\"\n",
    "\n",
    "    def aggregate(self, neighbors_targets,train_weigth):\n",
    "        \"\"\"Return the most common target label.\"\"\"\n",
    "        # print(\"Neighbors_target:\", neighbors_targets)\n",
    "        \n",
    "        # Inicialize um defaultdict com valor float\n",
    "        weighted_dict = defaultdict(float)\n",
    "        weight_sum = 0\n",
    "        # Percorra os vizinhos e atualize o dicionário ponderado\n",
    "        for target, weight in neighbors_targets:\n",
    "            weighted_dict[target] += weight\n",
    "            weight_sum += 1\n",
    "        \n",
    "        \n",
    "        print(weighted_dict.items())\n",
    "        for (target,_) in weighted_dict.items() :\n",
    "            weighted_dict[target] /= weight_sum\n",
    "            \n",
    "        weighted_dict = dict(weighted_dict)\n",
    "\n",
    "        if train_weigth == -1:\n",
    "            max_target = min(weighted_dict.items(), key=lambda item: item[1])\n",
    "        else:\n",
    "            max_target = max(weighted_dict.items(), key=lambda item: item[1])\n",
    "\n",
    "        # Desempacote a chave e o valor máximo\n",
    "        max_target_key, max_target_value = max_target\n",
    "        # print(\"target:\", max_target_key)\n",
    "        return max_target_key\n",
    "\n",
    "\n",
    "class KNNRegressor(KNNBase):\n",
    "    \"\"\"Nearest neighbors regressor.\"\"\"\n",
    "\n",
    "    def aggregate(self, neighbors_targets):\n",
    "        \"\"\"Return the mean of all targets.\"\"\"\n",
    "\n",
    "        return np.mean(neighbors_targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a empty list\n",
    "\n",
    "datasets = []\n",
    "datatraintest = []\n",
    "#ids = [37,163,40498,187,41,1527] #,1459\n",
    "#1527 -> Anomaly Detection Meta-Analysis Benchmarks\n",
    "#1459\n",
    "ids = [41]\n",
    "# push the fetch_and_prepare_dataset(dataset_id) with id 37 and 163\n",
    "for i in ids:\n",
    "    datasets.append(af.fetch_and_prepare_dataset(i))\n",
    "\n",
    "# split the datasets into train and test\n",
    "for i in range(len(ids)):\n",
    "    datatraintest.append(train_test_split(datasets[i][0], datasets[i][1], test_size=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [KNNClassifier(k=5)]\n",
    "classifiers_names = ['KNN Modified']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def classify_and_evaluate(classifier, X_train, X_test, y_train, y_test):\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    return accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1 -1  1  1 -1  1 -1  1 -1  1 -1 -1  1  1 -1  1  1  1  1  1 -1  1  1  1\n",
      " -1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1 -1  1 -1]\n",
      "dict_items([(0, 1.0), (1, 2.0)])\n",
      "dict_items([(0, 1.0), (1, 0.0)])\n",
      "dict_items([(0, 1.0), (1, 0.0)])\n",
      "dict_items([(0, 2.0), (1, 1.0)])\n",
      "dict_items([(0, 3.0)])\n",
      "dict_items([(0, 2.0), (1, 1.0)])\n",
      "dict_items([(2, 1.0), (0, 2.0)])\n",
      "dict_items([(0, 2.0), (1, 1.0)])\n",
      "dict_items([(2, 1.0), (0, 2.0), (1, 2.0)])\n",
      "dict_items([(2, 1.0), (1, 2.0), (0, 2.0)])\n",
      "dict_items([(0, 3.0), (2, 1.0), (1, 1.0)])\n",
      "dict_items([(0, 2.0), (1, 3.0)])\n",
      "dict_items([(0, 3.0), (2, 1.0), (1, 1.0)])\n",
      "dict_items([(2, 1.0), (0, 3.0), (1, 1.0)])\n",
      "dict_items([(0, 4.0), (2, 1.0)])\n",
      "dict_items([(0, 4.0), (2, 1.0)])\n",
      "dict_items([(0, 5.0)])\n",
      "dict_items([(0, 5.0)])\n",
      "dict_items([(0, 5.0)])\n",
      "dict_items([(0, 4.0), (2, 1.0)])\n",
      "dict_items([(0, 2.0), (1, 3.0)])\n",
      "dict_items([(1, 3.0), (0, 2.0)])\n",
      "dict_items([(0, 2.0), (1, 3.0)])\n",
      "dict_items([(0, 4.0), (1, 1.0)])\n",
      "dict_items([(1, 2.0), (3, 1.0)])\n",
      "dict_items([(1, 0.0), (3, 1.0)])\n",
      "dict_items([(1, 1.0)])\n",
      "dict_items([(1, 1.0)])\n",
      "dict_items([(1, 5.0)])\n",
      "dict_items([(1, 5.0)])\n",
      "dict_items([(1, 5.0)])\n",
      "dict_items([(0, 3.0), (1, 2.0)])\n",
      "dict_items([(4, 5.0)])\n",
      "dict_items([(4, 5.0)])\n",
      "dict_items([(4, 5.0)])\n",
      "dict_items([(4, 5.0)])\n",
      "dict_items([(1, 3.0), (0, 2.0)])\n",
      "dict_items([(1, 4.0), (0, 1.0)])\n",
      "dict_items([(1, 3.0), (0, 2.0)])\n",
      "dict_items([(0, 1.0), (1, 4.0)])\n",
      "dict_items([(5, -3.0), (1, 1.0), (0, 1.0)])\n",
      "dict_items([(5, -3.0), (1, 1.0), (0, 1.0)])\n",
      "dict_items([(5, -3.0), (0, 1.0), (1, 1.0)])\n",
      "dict_items([(0, 1.0), (1, 2.0)])\n",
      "dict_items([(4, 5.0)])\n",
      "dict_items([(4, 5.0)])\n",
      "dict_items([(4, 5.0)])\n",
      "dict_items([(4, 5.0)])\n",
      "dict_items([(0, 3.0), (1, 2.0)])\n",
      "dict_items([(0, 3.0), (1, 2.0)])\n",
      "dict_items([(0, 3.0), (1, 2.0)])\n",
      "dict_items([(0, 3.0), (1, 2.0)])\n",
      "dict_items([(0, 2.0), (1, 2.0), (2, 1.0)])\n",
      "dict_items([(0, 3.0), (1, 1.0), (2, 1.0)])\n",
      "dict_items([(0, 2.0), (1, 1.0), (2, 2.0)])\n",
      "dict_items([(0, 3.0), (1, 1.0), (2, 1.0)])\n",
      "dict_items([(1, 2.0), (0, 1.0)])\n",
      "dict_items([(1, 3.0)])\n",
      "dict_items([(1, 2.0), (0, 1.0)])\n",
      "dict_items([(1, 5.0)])\n",
      "dict_items([(1, 5.0)])\n",
      "dict_items([(1, 5.0)])\n",
      "dict_items([(1, 5.0)])\n",
      "dict_items([(1, 3.0), (0, 2.0)])\n",
      "dict_items([(0, 2.0), (1, 3.0)])\n",
      "dict_items([(0, 2.0), (1, 2.0), (2, 1.0)])\n",
      "dict_items([(0, 2.0), (1, 2.0), (2, 1.0)])\n",
      "dict_items([(1, 3.0), (0, 2.0)])\n",
      "dict_items([(0, 2.0), (1, 3.0)])\n",
      "dict_items([(0, 2.0), (1, 3.0)])\n",
      "dict_items([(0, 2.0), (1, 3.0)])\n",
      "dict_items([(0, 4.0), (1, 1.0)])\n",
      "dict_items([(0, 3.0), (2, 1.0), (1, 1.0)])\n",
      "dict_items([(0, 3.0), (2, 1.0), (1, 1.0)])\n",
      "dict_items([(0, 2.0), (2, 1.0), (1, 2.0)])\n",
      "dict_items([(0, 3.0), (1, 2.0)])\n",
      "dict_items([(0, 4.0), (1, 1.0)])\n",
      "dict_items([(0, 4.0), (1, 1.0)])\n",
      "dict_items([(0, 5.0)])\n",
      "dict_items([(0, 5.0)])\n",
      "dict_items([(5, -3.0), (3, -2.0)])\n",
      "dict_items([(5, -2.0), (3, -1.0), (2, 2.0)])\n",
      "dict_items([(5, -3.0), (3, -2.0)])\n",
      "dict_items([(1, 2.0), (3, -1.0), (0, 0.0)])\n",
      "dict_items([(1, 4.0), (2, 1.0)])\n",
      "dict_items([(1, 4.0), (2, 1.0)])\n",
      "dict_items([(1, 4.0), (0, 1.0)])\n",
      "dict_items([(1, 3.0), (2, 1.0), (0, 1.0)])\n",
      "dict_items([(0, 4.0), (2, 1.0)])\n",
      "dict_items([(2, 1.0), (0, 4.0)])\n",
      "dict_items([(0, 4.0), (2, 1.0)])\n",
      "dict_items([(2, 1.0), (0, 4.0)])\n",
      "dict_items([(1, 5.0)])\n",
      "dict_items([(1, 5.0)])\n",
      "dict_items([(1, 5.0)])\n",
      "dict_items([(1, 5.0)])\n",
      "dict_items([(0, 3.0), (2, 2.0)])\n",
      "dict_items([(0, 3.0), (2, 2.0)])\n",
      "dict_items([(0, 3.0), (2, 2.0)])\n",
      "dict_items([(0, 3.0), (2, 0.0)])\n",
      "dict_items([(1, 3.0), (2, 1.0), (0, 1.0)])\n",
      "dict_items([(1, 4.0), (2, 1.0)])\n",
      "dict_items([(1, 4.0), (2, 1.0)])\n",
      "dict_items([(1, 4.0), (0, 1.0)])\n",
      "dict_items([(0, 3.0), (2, 2.0)])\n",
      "dict_items([(0, 3.0), (2, 2.0)])\n",
      "dict_items([(0, 3.0), (2, 2.0)])\n",
      "dict_items([(0, 4.0), (2, 1.0)])\n",
      "dict_items([(1, 3.0), (2, 1.0), (0, 1.0)])\n",
      "dict_items([(1, 4.0), (0, 1.0)])\n",
      "dict_items([(0, 1.0), (1, 3.0), (2, 1.0)])\n",
      "dict_items([(1, 3.0), (0, 2.0)])\n",
      "dict_items([(2, 2.0), (0, 3.0)])\n",
      "dict_items([(0, 3.0), (2, 2.0)])\n",
      "dict_items([(0, 3.0), (2, 2.0)])\n",
      "dict_items([(0, 4.0), (2, 1.0)])\n",
      "dict_items([(0, 5.0)])\n",
      "dict_items([(0, 5.0)])\n",
      "dict_items([(0, 5.0)])\n",
      "dict_items([(0, 4.0), (2, 1.0)])\n",
      "dict_items([(1, 5.0)])\n",
      "dict_items([(1, 5.0)])\n",
      "dict_items([(1, 5.0)])\n",
      "dict_items([(1, 2.0), (2, 1.0), (0, 2.0)])\n",
      "dict_items([(1, 5.0)])\n",
      "dict_items([(1, 4.0), (0, 1.0)])\n",
      "dict_items([(1, 5.0)])\n",
      "dict_items([(1, 5.0)])\n",
      "dict_items([(0, 5.0)])\n",
      "dict_items([(0, 5.0)])\n",
      "dict_items([(0, 5.0)])\n",
      "dict_items([(0, 5.0)])\n",
      "dict_items([(0, 4.0), (1, 1.0)])\n",
      "dict_items([(0, 3.0), (1, 2.0)])\n",
      "dict_items([(0, 4.0), (1, 1.0)])\n",
      "dict_items([(0, 3.0), (1, 2.0)])\n",
      "dict_items([(1, 3.0), (0, 2.0)])\n",
      "dict_items([(1, 2.0), (0, 3.0)])\n",
      "dict_items([(1, 3.0), (0, 2.0)])\n",
      "dict_items([(1, 3.0)])\n",
      "dict_items([(0, 4.0), (2, 1.0)])\n",
      "dict_items([(0, 4.0), (2, 1.0)])\n",
      "dict_items([(0, 4.0), (2, 1.0)])\n",
      "dict_items([(0, 5.0)])\n",
      "dict_items([(2, 3.0), (1, 1.0), (0, 1.0)])\n",
      "dict_items([(2, 3.0), (0, 2.0)])\n",
      "dict_items([(2, 2.0), (0, 3.0)])\n",
      "dict_items([(2, 2.0), (0, 3.0)])\n",
      "dict_items([(0, 5.0)])\n",
      "dict_items([(0, 5.0)])\n",
      "dict_items([(0, 5.0)])\n",
      "dict_items([(0, 4.0), (2, 1.0)])\n",
      "dict_items([(0, 5.0)])\n",
      "dict_items([(0, 5.0)])\n",
      "dict_items([(0, 5.0)])\n",
      "dict_items([(1, 3.0), (0, 2.0)])\n",
      "dict_items([(0, 1.0), (1, 3.0), (2, 1.0)])\n",
      "dict_items([(0, 1.0), (1, 3.0), (2, 1.0)])\n",
      "dict_items([(1, 4.0), (0, 1.0)])\n",
      "dict_items([(1, 3.0), (0, 1.0), (2, 1.0)])\n",
      "dict_items([(5, -3.0), (4, -1.0), (0, 1.0)])\n",
      "dict_items([(5, -3.0), (4, -1.0), (0, 1.0)])\n",
      "dict_items([(5, -3.0), (4, -1.0), (0, 1.0)])\n",
      "dict_items([(5, -4.0), (1, 1.0)])\n",
      "dict_items([(1, 5.0)])\n",
      "dict_items([(1, 5.0)])\n",
      "dict_items([(1, 5.0)])\n",
      "dict_items([(1, 5.0)])\n",
      "dict_items([(4, -1.0), (0, 1.0), (1, 3.0)])\n",
      "dict_items([(4, -1.0), (1, 3.0), (0, 1.0)])\n",
      "dict_items([(4, -1.0), (3, -1.0), (1, 2.0), (0, 1.0)])\n",
      "dict_items([(4, -2.0), (3, -1.0), (1, 2.0)])\n",
      "[1, 1, 0, 0, 0, 1, 1, 1, 4, 1, 5, 4, 0, 0, 0, 1, 1, 1, 0, 0, 5, 1, 0, 1, 2, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 2, 2, 0, 0, 1, 5, 1, 4]\n",
      "Accuracy for dataset 0 with classifier KNN Modified: 0.5581395348837209\n",
      "[1 1 1 1 0 1 1 1 4 0 1 4 0 0 1 1 2 0 2 0 1 0 0 1 0 1 2 0 4 0 1 1 0 1 1 0 1\n",
      " 2 0 0 5 1 4]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(datasets)):\n",
    "    X_train, X_test, y_train, y_test = datatraintest[i]\n",
    "    for classifier, classifier_name in zip(classifiers, classifiers_names):\n",
    "        accuracy = classify_and_evaluate(classifier, X_train, X_test, y_train, y_test)\n",
    "        print(f'Accuracy for dataset {i} with classifier {classifier_name}: {accuracy}')\n",
    "        # 4.75\n",
    "        # 3 -> 1.88 -> 0.39\n",
    "        # 1 -> 0.95 -> 0.2\n",
    "        # 5 -> 1.92 -> \n",
    "    print(np.array(y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
